{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Network\n",
    "## This training is super super slow, it can take up hours!!!\n",
    "\n",
    "Originally by the Tensorflow authors, modified by Donald Li for Fall 2017\n",
    " \n",
    "This homework requires [tensorflow](https://www.tensorflow.org/install/), please make sure it is correctly installed before running the codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization and Preparing DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start a new session for tensorflow\n",
    "import tensorflow.compat.v1 as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#Download Mnist dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "tf.disable_eager_execution()\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#Initialize weight\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "#Initialize place holder\n",
    "x = tf.placeholder(tf.float32, shape=[None, 28,28])\n",
    "y_ = tf.placeholder(tf.int32, shape=[None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define what a convolutional layer and pooling layers. The following is the structure of the current convolutional network.\n",
    "<img width=\"300\" src='mnist_deep.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def avg_pool_2x2(x):\n",
    "  return tf.nn.avg_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = data[idx]\n",
    "    labels_shuffle = labels[idx]\n",
    "\n",
    "    return data_shuffle, labels_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to specify how many parameters there should be, and for the first convolution and pooling layer, what kind of how many units are there, and how they are being connected. Here, h is the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "#h_pool1 = avg_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can do similar thing to make the second layer of the network, it is important to notice that the number of input and output units are differnet in different layers, so when you want to decrease or increase the number of layer, you have to be careful about that. The output image size is 7*7 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "#h_pool2 = avg_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a fully-connected layer to allow processing on the entire image, here we have used a ReLu function. Then do the drop out process to prevent overfitting, finally there is a read out layer to get the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "y_onehot = tf.one_hot(y_,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we train and test the model. Instead of using least square, we are using cross entropy to be the term being oprtimized. We do not use a steepest gradient descent here, but using a ADAM alogrithm to do the optimization, in every 100 iterations, results will be printed out. For each batch of training, we feed in 50 images. This training is super super slow, it can take up hours!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining cross-entropy here\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_onehot, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_onehot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "#Do the training\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for i in range(5000):\n",
    "    batch = next_batch(50,x_train,y_train)\n",
    "    if i % 100 == 0:\n",
    "      train_accuracy = accuracy.eval(feed_dict={\n",
    "          x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "      print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "#Test the accuracy\n",
    "  print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "      x: x_test, y_: y_test, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression Model\n",
    "\n",
    "First, we have to initialize place holders and variables. \n",
    "It will be better to run the initialization and data preparation cell again before running the following!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize weights and bias for the regression model\n",
    "#Start a new session for tensorflow\n",
    "import tensorflow.compat.v1 as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#Download Mnist dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "tf.disable_eager_execution()\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#Initialize place holder\n",
    "x = tf.placeholder(tf.float32, shape=[None, 28,28])\n",
    "y_ = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class is predicted using a softmax model, and the loss function uses a cross entropy in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class prediction\n",
    "x_oneD = tf.reshape(x, [-1, 784])\n",
    "y = tf.matmul(x_oneD,W) + b\n",
    "y_onehot = tf.one_hot(y_,10)\n",
    "#Loss function\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_onehot, logits=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell do the training for the regression model, you can try to manipulate the number of training done to see what's happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the gradient decent training \n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "for _ in range(1000): #You may want to try 10,100,1000,10000\n",
    "  batch = next_batch(100,x_train,y_train)\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, calculate the prediction and see how off it is to the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the prediction\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_onehot,1))\n",
    "\n",
    "#Calculate and print the accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy is\", accuracy.eval(feed_dict={x: x_test, y_: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW\n",
    "\n",
    "1. Try both average pooling and max-pooling for the convolutional neural network, does the results agree on what you have discussed in Question 2-2? What is the difference between them? (5 points)\n",
    "2. Modify the code to make it into a one-layer network (it is very simple), but you just have to be careful about the dimension of each output layer to make sure they are correct, do you find a accuracy difference between one-layer convolutional network and a two-layer convolutional network? Report the results and explain (5 points)\n",
    "3. Run the simple softmax regression model and see how is the results different from the convolutional neural network? You should change the number of training and see how the accuaracy changes in the regression model. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
